{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Wikipedia for Names of Climbers and Mountaineers\n",
    "===\n",
    "\n",
    "If you've ever worked with Docker you might have been pleasantly surprised by container names such as `furious_einstein`, `agitated_curie`, `wizardly_lovelace`, and `romantic_darwin`. \n",
    "\n",
    "The code responsible for generating names such as these [(moby/names-generator.go)](https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go) works by pairing a long list of adjectives together with a list of last names. [<sup>1</sup>](#code-style)\n",
    "\n",
    "I wanted to write a tool that generates similar strings, but with a climbing flair, such as `beautiful_honnold`, `amazing_sharma`, and `cool_hill` and get the route setters at my local climbing gym to use it to name our indoor routes so me and my friends could have an easier time referring to routes when we aren't on site.\n",
    "\n",
    "However, I am much too lazy to write down all of these last names by hand. And instead, I wanted to automagically scrape as many of them as I could off of the internet instead. Conveniently, Wikipedia has a page listing several well-known [climbers and mountaineers](https://en.wikipedia.org/wiki/List_of_climbers_and_mountaineers) that we can, of course, scrape.\n",
    "\n",
    "# Getting the Raw Data\n",
    "\n",
    "The aforementioned Wikipedia page 'https://en.wikipedia.org/wiki/List_of_climbers_and_mountaineers' lists through climbers and mountaineers by their last name, from A to Z. \n",
    "\n",
    "To begin, we want to fetch the page and soupify it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "list_of_climbers_url = 'https://en.wikipedia.org/wiki/List_of_climbers_and_mountaineers'\n",
    "\n",
    "def get_html(url):\n",
    "    return requests.get(url).text\n",
    "\n",
    "soup = BeautifulSoup(get_html(list_of_climbers_url), 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you visit the [Wikipedia-page](https://en.wikipedia.org/wiki/List_of_climbers_and_mountaineers), and inspect its source, you'd notice that it is composed of a bunch of unordered lists, and that the unordered lists that are interesting to us are all preceeded by an `h2` with a `span` having an `id`such that its length is equal to `1` **and** it is an uppercase letter.\n",
    "\n",
    "Thus, if we iterate across all `h2`s that satisfies these conditions we'll be able to grab all list entries within these unordered lists. Each such list entry is, conveniently, a climber.\n",
    "\n",
    "We could write some code that grabs all of these list entires in the following manner,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # So we can check if the id is an uppercase letter\n",
    "\n",
    "climber_lis = []\n",
    "\n",
    "for h2 in soup.find_all('h2'):\n",
    "    if h2.span: # Will be None (False) if there isn't a span to grab onto\n",
    "        \n",
    "        # Is a single letter in the range A..Z\n",
    "        if len(h2.span['id']) == 1 and h2.span['id'] in string.ascii_uppercase:\n",
    "                climber_lis.append(h2.find_next('ul').find_all('li'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives us a nested list of lists, such that the first sublist all contain climbers with have a last name starting with the letter `A`, the second sublist contains climbers that have a last name that starts with the letter `B`, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a> (1906–1992) Russia, climbed <a href=\"/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a> (1934) and <a href=\"/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a> (1936)</li>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climber_lis[0][0] # The 'li' for Vitaly Abalakov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Samina_Baig\" title=\"Samina Baig\">Samina Baig</a> - <a href=\"/wiki/Gilgit-Baltistan\" title=\"Gilgit-Baltistan\">Gilgit-Baltistan</a>, 3rd Pakistani and only Pakistani woman to climb <a href=\"/wiki/Mount_Everest\" title=\"Mount Everest\">Mount Everest</a></li>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climber_lis[1][0] # The 'li' for Samina Baig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while the code is short, and complete with some helpful comments, it could be made more legible with some convenience functions.\n",
    "\n",
    "Adding such functions in rarely takes a long amount of time, and while it might interrupt our flow and cadence at times, it has been my experience that whenever one optimizes for development speed in the short-term it has a long-term cost whenever one has to re-read the code or change it. I try to remind myself of the [Parable of the Road Line Painter](https://davembush.github.io/the-parable-of-the-road-line-painter/) whenever I find myself blazing ahead to quickly.\n",
    "\n",
    "Even with a small project such as this one, it is usually worth the effort. The following convenience functions are adequate enough to improve the legibility,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a> (1906–1992) Russia, climbed <a href=\"/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a> (1934) and <a href=\"/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a> (1936)</li>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def has_span_attribute(h2):\n",
    "    return h2.span is not None\n",
    "\n",
    "def is_single_letter(s):\n",
    "    return len(s) == 1\n",
    "\n",
    "def is_uppercase_letter(c):\n",
    "    import string\n",
    "    return c in string.ascii_uppercase\n",
    "\n",
    "def find_all_climbers(soup):\n",
    "    climber_lis = []\n",
    "    \n",
    "    for h2 in soup.find_all('h2'):\n",
    "        if has_span_attribute(h2):\n",
    "            span_id = h2.span['id'] \n",
    "            if is_single_letter(span_id) and is_uppercase_letter(span_id):\n",
    "                climber_lis.append(h2.find_next('ul').find_all('li'))\n",
    "                \n",
    "    return climber_lis\n",
    "                \n",
    "climber_lis = find_all_climbers(soup)\n",
    "climber_lis[0][0] # Still works the same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the Data\n",
    "\n",
    "The nested structure, wherein the list is grouped by letter, is not something we need as we want to operate on the entire set of climbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a> (1906–1992) Russia, climbed <a href=\"/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a> (1934) and <a href=\"/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a> (1936)</li>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_list(l):\n",
    "    from functools import reduce\n",
    "    import operator\n",
    "    \n",
    "    return reduce(operator.add, l)\n",
    "\n",
    "climber_lis = flatten_list(climber_lis)\n",
    "climber_lis[0] # Still Vitaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Unwanted (Unfortunate) Entries\n",
    "\n",
    "Now we want to filter away climbers with more complex last names such as those that,\n",
    "\n",
    "1. Contain non-ascii characters, as not all label makers support those characters.\n",
    "2. Consists of several \"words\" as they are presumably,\n",
    "    2.1. Long, \n",
    "    2.2. and do not fit the Docker container name schema.\n",
    "    \n",
    "and so we want to create a predicate function for that. \n",
    "\n",
    "But first, how do we extract the complete name of the climber from these list entires? First recall that `climber_lis` is a (now flat) list of all climbers. Most of these entires start off with a link to the respective Wikipedia page for that climber and the link names are conveninently the name of that particular climber.\n",
    "\n",
    "Therefore, by retaining all `li`s that have a link in them we should have an easy time grabbing the names of the climbers to which we later apply our predicate rule. And so, we begin by filtering all list entries without a link,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_a_href(climber_li):\n",
    "    return climber_li.a is not None\n",
    "\n",
    "climber_lis = list(filter(contains_a_href, climber_lis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining entries, we will assume then that the full name of the climber is in the text portion of the link, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_name(climber_li):\n",
    "    assert(contains_a_href(climber_li))\n",
    "    return climber_li.a.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the predicate for whether or not the last name of a climber is simple enough for us to use or not,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_be_printed_on_any_label_maker(s):\n",
    "    return s.isascii()\n",
    "\n",
    "def consists_of_two_words(s):\n",
    "    return len(s.split(' ')) == 2\n",
    "\n",
    "def last_name(full_name):\n",
    "    return full_name.split(' ')[1]\n",
    "\n",
    "def has_simple_lastname(full_name):\n",
    "    return consists_of_two_words(full_name) and can_be_printed_on_any_label_maker(last_name(full_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if all we wanted to do was grab all the last names, toss it into a list, and start pairing it with a bunch of adjectives we'd be almost done at this point. Really, this is all we'd have left to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abalakov', 'Abalakov', 'Agarwal', 'Allain', 'Almer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_names = map(full_name, climber_lis)\n",
    "printable_names = filter(has_simple_lastname, full_names)\n",
    "all_last_names = list(map(last_name, printable_names))\n",
    "assert(len(last_name) == 1 for last_name in all_last_names)\n",
    "\n",
    "# You can print out all_last_names if you want, but I'm just going to show you a few values for the benefit of\n",
    "# static rendering on Github, \n",
    "all_last_names[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all you want are the last names, you are done here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grabbing More than Just Names\n",
    "\n",
    "But, if you have a look at [(moby/names-generator.go)](https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go) you'd see that each person there has an associated description, like so,\n",
    "\n",
    "```\n",
    "// names-generator.go\n",
    "...\n",
    "// Sophie Wilson designed the first Acorn Micro-Computer and the instruction set for ARM processors. https://en.wikipedia.org/wiki/Sophie_Wilson\n",
    "\"wilson\",\n",
    "\n",
    "// Jeannette Wing - co-developed the Liskov substitution principle. - https://en.wikipedia.org/wiki/Jeannette_Wing\n",
    "\"wing\",\n",
    "\n",
    "// Steve Wozniak invented the Apple I and Apple II. https://en.wikipedia.org/wiki/Steve_Wozniak\n",
    "\"wozniak\",\n",
    "...\n",
    "```\n",
    "\n",
    "and right now, in our `li`s we have these types of descriptions.\n",
    "\n",
    "Wouldn't it be neat if we could keep the descriptions around so that when we render out our route names later we can render out the description for that climber as well? I think so, and I'd very much like it if we did just that. And to accomplish this, we have to put in a bit more effort into it. We want to perform the same filtering as we did, but keep around the original `li` so we can grab the description later.\n",
    "\n",
    "This is something we can accomplish easily with a dictionary comprehension,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Tommy_Caldwell\" title=\"Tommy Caldwell\">Tommy Caldwell</a> (born 1978) US, rock climber, free climbed Nose of El Capitan</li>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_last_name_to_li(climber_lis):\n",
    "    return {last_name(full_name(li)): li for li in climber_lis if has_simple_lastname(full_name(li))}\n",
    "    \n",
    "last_names_with_li = map_last_name_to_li(climber_lis)\n",
    "\n",
    "# Again, this is a pretty \"big\" dataset, in the sense that it looks kind of bad if we render it all out, so\n",
    "# let's just look at a single item to see what we have,\n",
    "last_names_with_li['Caldwell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But! With this approach we run the risk of overwriting entries within the dictionary. In fact, we lose out on a climbing legend with the above approach. You'd expect any set of notable climbers to include Lynn Hill but with our dictionary comprehension that entry is written over with Sandy Hill,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Sandy_Hill_(mountaineer)\" title=\"Sandy Hill (mountaineer)\">Sandy Hill</a> (born 1955) US, Seven Summits</li>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_names_with_li['Hill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can either retain the same indexing style, viz. by last name which could be solved as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a>,\n",
       " ' (born 1961) US, first free ascent The Nose on El Capitan, ',\n",
       " <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a>,\n",
       " ' (1993)',\n",
       " <a href=\"/wiki/Sandy_Hill_(mountaineer)\" title=\"Sandy Hill (mountaineer)\">Sandy Hill</a>,\n",
       " ' (born 1955) US, Seven Summits']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_last_name_to_li(climber_lis):\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # 1. Define the default value when indexing to be an empty list\n",
    "    last_name_to_li_map = defaultdict(lambda: [])\n",
    "    \n",
    "    for li in climber_lis:\n",
    "        first_and_last_name = full_name(li)\n",
    "        if has_simple_lastname(first_and_last_name):\n",
    "            key = last_name(first_and_last_name)\n",
    "            \n",
    "            # 2. So we can use the key here, regardless of having seen the key before\n",
    "            last_name_to_li_map[key] += li\n",
    "            \n",
    "    return last_name_to_li_map\n",
    "\n",
    "last_names_with_li = map_last_name_to_li(climber_lis)\n",
    "\n",
    "# Then we'd get a list returned instead,\n",
    "last_names_with_li['Hill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably, this is fine. While the issue is handled, we ask ourselves if the \"solution\" as it were is justified. Personally, I'd argue that it is not justified. How so? For there is no apparent reason for us to have modelled our data in such a way that we index using last names alone and from there on have to operate on the resulting list. Especially not when the majority of the keys within the dictionary will map to a list with a single entry.\n",
    "\n",
    "This dichotomy serves to guide us along further toward a more prudent design that is more cohesive. It would be more coherent if when we index our dictionary we get a single value, always. And we accomplish that by allowing the full name of the climber to be our key. And so, rather than invoking `map_last_name_to_li` we write out a function for mapping the name of the climber to the pertinent `li`. And so we may now again utilize a dict-comprehension for this, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_name_to_li(climber_lis):\n",
    "    return {full_name(li): li for li in climber_lis if has_simple_lastname(full_name(li))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we progress as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a> (born 1961) US, first free ascent The Nose on El Capitan, <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a> (1993)</li>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_with_li = map_name_to_li(climber_lis)\n",
    "\n",
    "names_with_li['Lynn Hill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not run the above code, it outputs the following,\n",
    "\n",
    "```\n",
    "<li><a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a> (born 1961) US, first free ascent The Nose on El Capitan, <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a> (1993)</li>\n",
    "```\n",
    "\n",
    "and what we would like it to output is a tuple of two elements. Namely, we want to create a tuple consisting of\n",
    "\n",
    "1. The link target, i.e. `'/wiki/Lynn_Hill'`, and\n",
    "2. the complete contents of the `li`, i.e. `'a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a> (born 1961) US, first free ascent The Nose on El Capitan, <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a> (1993)'`.\n",
    "\n",
    "How do we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing the Link Target\n",
    "\n",
    "Grabbing the link target is super easy, almost not worth a subsection to be honest,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Lynn_Hill'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_with_li['Lynn Hill'].a['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing the `li` Contents\n",
    "\n",
    "Getting the contents is _almost_ just as easy, we notice that the below code returns a list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a>,\n",
       " ' (born 1961) US, first free ascent The Nose on El Capitan, ',\n",
       " <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a>,\n",
       " ' (1993)']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_with_li['Lynn Hill'].contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which seems like it is almost what we want, just not quite. The keen observer would note that the first element is rendered without surrounding quotes, and so it clearly this is not just a list of strings. But what is it a list of then? By mapping the `type` function across the list we can figure this out,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[bs4.element.Tag,\n",
       " bs4.element.NavigableString,\n",
       " bs4.element.Tag,\n",
       " bs4.element.NavigableString]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(type, names_with_li['Lynn Hill']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, it's a list of BeautifulSoup objects. Conveniently for us, the BeautifulSoup documentation goes into how to get the raw HTML of any `BeautifulSoup` object [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#non-pretty-printing), we simply need to map the `str` function over the elements and join them together, i.e. we want to do the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a> (born 1961) US, first free ascent The Nose on El Capitan, <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a> (1993)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(list(map(str, names_with_li['Lynn Hill'].contents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing Them Both\n",
    "\n",
    "And now we want to do both these things for all elements in the `names_with_li`-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/wiki/Lynn_Hill',\n",
       " '<a href=\"/wiki/Lynn_Hill\" title=\"Lynn Hill\">Lynn Hill</a> (born 1961) US, first free ascent The Nose on El Capitan, <a href=\"/wiki/Yosemite_Valley\" title=\"Yosemite Valley\">Yosemite</a> (1993)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def raw_html(list_of_soup_objects):\n",
    "    return ''.join(map(str, list_of_soup_objects))\n",
    "\n",
    "def href(li):\n",
    "    return li.a['href']\n",
    "\n",
    "def description(li):\n",
    "    return raw_html(li.contents)\n",
    "\n",
    "def pair_href_with_description(names_with_lis):\n",
    "    return {full_name: (href(li), description(li)) for full_name, li in names_with_li.items()}\n",
    "\n",
    "names_with_descriptions = pair_href_with_description(names_with_li)\n",
    "names_with_descriptions['Lynn Hill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, that is exactly what we want! But we have a problem. Can you spot it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepend `href`s with `https://en.wikipedia.org/`\n",
    "\n",
    "That's right, all of our `href`s are relative to Wikipedia! Using `/wiki/Lynn_Hill` later as part of our own `href` won't redirect our website visitors anywhere. And, in the descriptions for certain climbers other Wikipedia pages are linked as well, for instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/wiki/Alex_Honnold',\n",
       " '<a href=\"/wiki/Alex_Honnold\" title=\"Alex Honnold\">Alex Honnold</a> (born 1985) US, free solo of <a href=\"/wiki/Half_Dome\" title=\"Half Dome\">Half Dome</a> northwest face (2008), Moonlight Buttress in <a href=\"/wiki/Zion_National_Park\" title=\"Zion National Park\">Zion National Park</a> (2008), and Freerider on El Capitan (2017)')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_with_descriptions['Alex Honnold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, we have to prepend our `href`s with `https://en.wikipedia.org/` before we can consider ourselves as being done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: String Replace for Honor and Glory\n",
    "\n",
    "One very simple approach is just replace the string `/wiki/` with `https://en.wikipedia.org/wiki/` wherever it occurs. The only drawback to this method is that we could easily forget to modify both elements of all of the tuples _if_ what we was working on was slightly more complex, but here it is not really an issue,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://en.wikipedia.org/wiki/Alex_Honnold',\n",
       " '<a href=\"https://en.wikipedia.org/wiki/Alex_Honnold\" title=\"Alex Honnold\">Alex Honnold</a> (born 1985) US, free solo of <a href=\"https://en.wikipedia.org/wiki/Half_Dome\" title=\"Half Dome\">Half Dome</a> northwest face (2008), Moonlight Buttress in <a href=\"https://en.wikipedia.org/wiki/Zion_National_Park\" title=\"Zion National Park\">Zion National Park</a> (2008), and Freerider on El Capitan (2017)')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_wiki_hrefs(names_with_descriptions):\n",
    "    def _expand_wiki_hrefs_(some_tuple_of_strings):\n",
    "        def __expand_wiki_hrefs__(some_string):\n",
    "            return some_string.replace('/wiki/', 'https://en.wikipedia.org/wiki/')\n",
    "\n",
    "        t = some_tuple_of_strings # To cut down on visual noise\n",
    "        return (__expand_wiki_hrefs__(t[0]), __expand_wiki_hrefs__(t[1]))\n",
    "    \n",
    "    return {full_name: _expand_wiki_hrefs_(v) for full_name, v in names_with_descriptions.items()}\n",
    "    \n",
    "names_with_descriptions_m1 = expand_wiki_hrefs(names_with_descriptions)\n",
    "names_with_descriptions_m1['Alex Honnold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Add an Intermediate Processing Step\n",
    "\n",
    "A solution that does not risk us forgetting to modify some data is to step back, and add an intermediate step to our processing after we flattened the list of `li`s. This method has the advantage of ensuring that all `href`s are modified in a consistent manner. But as we will see later, it has its own set of drawbacks.\n",
    "\n",
    "Recall that up until this point we have performed the following operations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "climber_lis = find_all_climbers(soup)\n",
    "climber_lis = flatten_list(climber_lis)\n",
    "climber_lis = list(filter(contains_a_href, climber_lis))\n",
    "names_with_li = map_name_to_li(climber_lis)\n",
    "names_with_descriptions = pair_href_with_description(names_with_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems prudent then to add this interim processing step after we filtered out the list entries that do not contain a link at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to refresh our memory, recall how an element in `climber_lis` looks like,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a> (1906–1992) Russia, climbed <a href=\"/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a> (1934) and <a href=\"/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a> (1936)</li>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climber_lis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can conveniently use the `find_all`-method to list all the `a`-elements within each of these `li`s,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a>,\n",
       " <a href=\"/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a>,\n",
       " <a href=\"/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_all_links(li):\n",
    "    return li.find_all('a')\n",
    "\n",
    "list_all_links(climber_lis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every element returned by `list_all_links` is such that it is a `BeautifulSoup`-object, specifically a `bs4.element.Tag`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "all(type(a) == bs4.element.Tag for a in list_all_links(climber_lis[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By virtue of being an object, each such element is inherently stateful, and so we may modify them in-place and prepend `https://en.wikipedia.org` to the links,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepend_href_with_prefix(a, prefix='https://en.wikipedia.org'):\n",
    "    a['href'] = prefix + a['href']\n",
    "    return a\n",
    "    \n",
    "list(map(prepend_href_with_prefix, list_all_links(climber_lis[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as stated, this will modify the actual entries within the list, in-place. Notice how each `href`-target is prefixed with `https://en.wikipedia.org`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><a href=\"https://en.wikipedia.org/wiki/Vitaly_Abalakov\" title=\"Vitaly Abalakov\">Vitaly Abalakov</a> (1906–1992) Russia, climbed <a href=\"https://en.wikipedia.org/wiki/Lenin_Peak\" title=\"Lenin Peak\">Lenin Peak</a> (1934) and <a href=\"https://en.wikipedia.org/wiki/Khan_Tengri\" title=\"Khan Tengri\">Khan Tengri</a> (1936)</li>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climber_lis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not like this method. Why? Because there are usually a lot of pitfalls involved with state. For instance, if we now iterated over `climber_lis` to prepend the prefixes we'd find that `climber_lis[0]` would be prepended with the prefixes twice.\n",
    "\n",
    "And thus we'd have to start anew, and _then_ do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://en.wikipedia.org/wiki/Alex_Honnold',\n",
       " '<a href=\"https://en.wikipedia.org/wiki/Alex_Honnold\" title=\"Alex Honnold\">Alex Honnold</a> (born 1985) US, free solo of <a href=\"https://en.wikipedia.org/wiki/Half_Dome\" title=\"Half Dome\">Half Dome</a> northwest face (2008), Moonlight Buttress in <a href=\"https://en.wikipedia.org/wiki/Zion_National_Park\" title=\"Zion National Park\">Zion National Park</a> (2008), and Freerider on El Capitan (2017)')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climber_lis = find_all_climbers(soup)\n",
    "climber_lis = flatten_list(climber_lis)\n",
    "climber_lis = list(filter(contains_a_href, climber_lis))\n",
    "\n",
    "for li in climber_lis:\n",
    "    # Calling list here forces the evaluation of the map-expression which is otherwise lazy\n",
    "    list(map(prepend_href_with_prefix, list_all_links(li)))\n",
    "\n",
    "names_with_li = map_name_to_li(climber_lis)\n",
    "names_with_descriptions_m2 = pair_href_with_description(names_with_li)\n",
    "names_with_descriptions_m2['Alex Honnold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both methods produce identical results,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_with_descriptions_m1['Alex Honnold'] == names_with_descriptions_m2['Alex Honnold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping out our results to JSON\n",
    "\n",
    "And so, we dump out our data to JSON and leave this notebook behind,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('climbers.json', 'w') as json_file:\n",
    "    json.dump(names_with_descriptions_m1, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<span id=\"code-style\">Note 1:</span> To me, this bit of code is _just_ right. No unnecessary complexity, it's simple and to the point and anyone with a modicum of programming experience could understand it, even if they do not know Go: the programming language that the code is written in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
